 
 '? ";/. ;.m,. #################################
# REPLICATOR.PROPERTIES.DIRECT  #
#################################
#
# This file contains properties for MySQL replication.
#
# NOTE TO ALL USERS:  Blank property values are assigned as empty strings. 
# To assign the default value, comment out the key=value assignment.
#
# NOTE TO WINDOWS USERS:  Single backslash characters are treated as escape 
# characters.  You must use forward slash (/) or double backslashes in file 
# names. 

#################################
# GLOBAL REPLICATOR PARAMETERS  #
#################################

# RMI port to use for advertising JMX services
replicator.rmi_port=<%= ENV['OPENSHIFT_TUNGSTEN_REPL_RMI_PORT'] %>
replicator.rmi_host=<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>

# Database connection information. 
replicator.global.db.host=<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>
replicator.global.db.port=<%= ENV['OPENSHIFT_TUNGSTEN_DB_PORT'] %>
replicator.global.db.user=<%= ENV['OPENSHIFT_MYSQL_DB_USERNAME'] %>
replicator.global.db.password=<%= ENV['OPENSHIFT_MYSQL_DB_PASSWORD'] %>

# Properties for extractors only.  This supports direct piplines.
replicator.global.extract.db.host=<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>
replicator.global.extract.db.port=<%= ENV['OPENSHIFT_TUNGSTEN_DB_PORT'] %>
replicator.global.extract.db.user=<%= ENV['OPENSHIFT_MYSQL_DB_USERNAME'] %>
replicator.global.extract.db.password=<%= ENV['OPENSHIFT_MYSQL_DB_PASSWORD'] %>

# Replicator role.  You must specify a value that corresponds to a 
# pipeline name like master, slave, direct, etc.  There is no default 
# for this value--it must be set or the replicator will not go online.  
replicator.role=<%= ENV['TUNGSTEN_REPL_ROLE'] %>

# If the following property is true, we are replacing native slave 
# replication.  We start extraction from the MySQL slave position 
# and write slave restart coordinates each time the replicator goes 
# offline cleanly.  It should be false in all other cases. 
replicator.nativeSlaveTakeover=<%= ENV['TUNGSTEN_REPL_SVC_NATIVE_SLAVE_TAKEOVER'] %>

# URI to which we connect when this replicator is a slave.   
replicator.master.connect.uri=

# URI for our listener when we are acting as a master.  Slaves 
# use this as their connect URI.
replicator.master.listen.uri=thl://<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>:<%= ENV['OPENSHIFT_TUNGSTEN_REPL_MASTER_LISTEN_PORT'] %>/
replicator.master.listen.proxy.port=<%= ENV['OPENSHIFT_TUNGSTEN_REPL_MASTER_PROXY_PORT'] %>
    

# Force replicator to check if THL is in sync with database. 
# This check is enabled by default. Set the following property 
# to false in order to disable the check.
# replicator.master.thl_check=true

# Replicator auto-enable.  If true, replicator automatically goes online 
# at start-up time. 
replicator.auto_enable=<%= ENV['TUNGSTEN_REPL_AUTOENABLE'] %>

# Determines whether replication service is started up
# as a thread internal to the ReplicationServiceManager or
# whether it runs in a separate, detached JVM.
replicator.detached=false

# Source ID. This required parameter is used to identify replication
# event source.  It must be unique for each replicator node.
replicator.source_id=<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>

# Site to which the replicator belongs.
site.name=default

# Cluster name to which the replicator belongs.
cluster.name=<%= ENV['TUNGSTEN_CLUSTERNAME'] %>

# Replication service type.  Values are 'remote' or 'local'.  Remote 
# remote services are bringing in transactions from another master. 
replicator.service.type=<%= ENV['TUNGSTEN_REPL_SVC_SERVICE_TYPE'] %>

# Name of this replication service.  
service.name=<%= ENV['TUNGSTEN_SERVICE_DEPLOYMENT_SERVICE'] %>

# Whether to allow comments in SQL statements to show the service name.  
# This must be set to avoid multi-master replication loops if updates
# from one master go into the binlog of another master.
replicator.service.comments=<%= ENV['TUNGSTEN_SERVICE_REPL_SVC_ENABLE_MASTER_SERVICE_COMMENTS'] %>

# Whether to log slave updates or not.  This is normally false but can 
# be enabled so that updates inserted into the slave can be extracted again
# by another replicator.  This can be useful for multi-master and some 
# chained replication topologies.  
replicator.log.slave.updates=<%= ENV['TUNGSTEN_LOG_SLAVE_UPDATES'] %>

# Whether slave updates have a privileged account for updates.  Certain 
# stores such as Amazon RDS do not allow slave to have superuser privileges. 
# If this values is false we try to apply using non-superuser capabilites. 
replicator.privileged.slave.update=<%= ENV['TUNGSTEN_SLAVE_PRIVILEGED_UPDATES'] %>

# Name of the local replication service.  This parameter must be set when 
# performing bi-directional replication using a remote slave.  Events 
# generated by this service are dropped, thereby preventing replication 
# loops.  It may not be the same as the value of service.name if the service
# is remote. 
local.service.name=<%= ENV['TUNGSTEN_SERVICE_DSNAME'] %>

# Schema to store Replicator catalog tables. 
replicator.schema=<%= ENV['TUNGSTEN_REPL_SVC_SCHEMA'] %>

# Engine used for Replicator catalog tables. If it is undefined or empty, 
# tungsten will use mysql default storage engine. By default,
# it is set to use innodb.
replicator.table.engine=<%= ENV['TUNGSTEN_REPL_SVC_TABLE_ENGINE'] %>

# Global queue size for pipelines.  This defines the number of events
# buffered between stages.  Values greater than 1 improve performance
# dramatically but mean that you need to have enough heap memory to
# handle blobs and large transaction fragments.
replicator.global.buffer.size=<%= ENV['TUNGSTEN_REPL_BUFFER_SIZE'] %>

# For parallel replication we have a global apply thread count. 1 means
# that apply is single threaded. 
replicator.global.apply.channels=<%= ENV['TUNGSTEN_REPL_SVC_CHANNELS'] %>

# Policy for shard assignment based on default database.  If 'stringent', use
# default database only if SQL is recognized.  For 'relaxed' always use the 
# default database if it is available. 
replicator.shard.default.db=<%= ENV['TUNGSTEN_REPL_SVC_SHARD_DEFAULT_DB'] %>

# Used by manager to create datasources dynamically
replicator.resourceJdbcUrl=<%= ENV['TUNGSTEN_APPLIER_REPL_DBJDBCURL'] %>
replicator.resourceJdbcDriver=<%= ENV['TUNGSTEN_APPLIER_REPL_DBJDBCDRIVER'] %>
replicator.resourceVendor=<%= ENV['TUNGSTEN_APPLIER_REPL_DBJDBCVENDOR'] %>
replicator.resourcePrecedence=99
replicator.vipInterface=eth0:0
replicator.vipAddress=192.168.0.1
replicator.resourceLogPattern=mysql-bin
replicator.resourceLogDir=<%= ENV['OPENSHIFT_TUNGSTEN_DIR'] %>/opt/continuent/log
replicator.resourcePort=<%= ENV['OPENSHIFT_TUNGSTEN_DB_PORT'] %>
replicator.resourceDataServerHost=<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>

##########################
# OPEN REPLICATOR PLUGIN #
##########################

# Available OpenReplicator providers
replicator.plugin.tungsten=com.continuent.tungsten.replicator.management.tungsten.TungstenPlugin
replicator.plugin.script=com.continuent.tungsten.replicator.management.script.ScriptPlugin

# Chosen OpenReplicator provider
replicator.plugin=tungsten

#################################
# REPLICATOR PIPELINES          #
#################################

# Generic pipelines.
replicator.pipelines=master,slave,relay

# MASTER PIPELINE:  Two stages with an intervening queue:  extract from binlog
# and place events in THL.
replicator.pipeline.master=binlog-to-q,q-to-thl
replicator.pipeline.master.stores=thl,queue
replicator.pipeline.master.services=channel-assignment

replicator.stage.binlog-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.binlog-to-q.extractor=dbms
replicator.stage.binlog-to-q.applier=queue
replicator.stage.binlog-to-q.filters=<%= ENV['TUNGSTEN_REPL_SVC_EXTRACTOR_FILTERS'] %>

replicator.stage.q-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.q-to-thl.extractor=queue
replicator.stage.q-to-thl.applier=thl-applier
replicator.stage.q-to-thl.filters=<%= ENV['TUNGSTEN_REPL_SVC_THL_FILTERS'] %>
replicator.stage.q-to-thl.blockCommitRowCount=${replicator.global.buffer.size}

# SLAVE PIPELINE:  three stages:  extract from remote THL to local THL;
# extract from local THL to queue; apply from queue to DBMS.
replicator.pipeline.slave=remote-to-thl,thl-to-q,q-to-dbms
replicator.pipeline.slave.stores=thl,parallel-queue
replicator.pipeline.slave.services=channel-assignment
replicator.pipeline.slave.syncTHLWithExtractor=false

replicator.stage.remote-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.remote-to-thl.extractor=thl-remote
replicator.stage.remote-to-thl.applier=thl-applier
replicator.stage.remote-to-thl.filters=

replicator.stage.thl-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.thl-to-q.extractor=thl-extractor
replicator.stage.thl-to-q.applier=parallel-q-applier
replicator.stage.thl-to-q.blockCommitRowCount=${replicator.global.buffer.size}

replicator.stage.q-to-dbms=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.q-to-dbms.extractor=parallel-q-extractor
replicator.stage.q-to-dbms.applier=dbms
replicator.stage.q-to-dbms.filters=<%= ENV['TUNGSTEN_REPL_SVC_APPLIER_FILTERS'] %>
replicator.stage.q-to-dbms.taskCount=${replicator.global.apply.channels}
replicator.stage.q-to-dbms.blockCommitRowCount=<%= ENV['TUNGSTEN_REPL_SVC_APPLIER_BUFFER_SIZE'] %>

# RELAY PIPELINE: The relay pipeline is the same as the slave at present but
# may diverge at some point.  And this pipeline is required to support a
# replicator with role 'relay'
replicator.pipeline.relay=remote-to-thl,thl-to-q,q-to-dbms
replicator.pipeline.relay.stores=thl,parallel-queue
replicator.pipeline.relay.services=channel-assignment
replicator.pipeline.relay.syncTHLWithExtractor=false

#################################
# TRANSACTION HISTORY LOG (THL) #
#################################

# NOTE:  If you run multiple replication services, beware of collisions 
# on log file names and THL lister ports. 

# Set the THL storage implementation class.  All replicators now use
# a single implementation. 
replicator.store.thl=com.continuent.tungsten.replicator.thl.THL

# All THL implementations require database access to store metadata. 
replicator.store.thl.url=<%= ENV['TUNGSTEN_APPLIER_REPL_DBTHLURL'] %>
replicator.store.thl.user=${replicator.global.db.user}
replicator.store.thl.password=${replicator.global.db.password}

# Uncomment the following properties to control disk log storage location
# and size of files.  These are default values.  NOTE:  naming the logs by 
# service name is critical to prevent replication services from overwriting
# other service logs. 
replicator.store.thl.log_dir=<%= ENV['TUNGSTEN_SERVICE_REPL_LOG_DIR'] %>
replicator.store.thl.log_file_size=<%= ENV['TUNGSTEN_REPL_THL_LOG_FILE_SIZE'] %>

# The following property tells the THL server how long, as a maximum,
# in milliseconds, to block during an accept() call.  This interval, in turn
# will be directly factored into the amount of time a replicator will
# take to transition to the offline state on systems whose
# kernels or Java runtime won't allow for the interruption of a blocking
# accept() call.  If this interval is too short, it may result in problems 
# with new client connections, and if too long, will result in excessively
# slow transitions to offline etc.
replicator.thl.server.accept.timeout=5000

# Change the following property to control the buffer size used for THL
# I/O operations.  The default of 128k seems to work well. 
replicator.store.thl.bufferSize=131072

# The flush interval is the number of milliseconds that log writes may delay
# before being forced to storage.  0 means that every flush call flushes 
# immediately.  Larger values can help buffer writes but add latency to 
# pipelines.  This value only takes effect when fsyncOnFlush=true. 
replicator.store.thl.flushIntervalMillis=500

# Flush operations normally release writes to the OS but do
# not force them to storage.  You may request a full fsync on each flush by
# setting the following property to true.  This makes log updates more
# durable.  If you do this the flush interval should be relatively large to
# avoid impacting overall throughput or you should put the log on  
# high-performance storage to reduce fsync overhead.
replicator.store.thl.fsyncOnFlush=<%= ENV['TUNGSTEN_REPL_THL_LOG_FSYNC'] %>

# To drop log files after a certain period, set the retention to an interval
# which is <number>{d|h|m|s}, where the letters stand for days, hours, minutes,
# or seconds respectively.  If unset logs are retained indefinitely.
replicator.store.thl.log_file_retention=<%= ENV['TUNGSTEN_REPL_THL_LOG_RETENTION'] %>

# The THL serialization for events is pluggable.  The default is Protobuf
# serialization which is relatively fast and compact.  Java serialization
# is also provided but is experimental.
replicator.store.thl.event_serializer=com.continuent.tungsten.replicator.thl.serializer.ProtobufSerializer

# The disk log can compute checksums automatically on log records.  This
# is enabled by default but can cut performance by 50% or more.  Set false
# for faster log performance or to suppress reading *and* writing of checksums.
replicator.store.thl.doChecksum=<%= ENV['TUNGSTEN_REPL_THL_DO_CHECKSUM'] %>

# Maximum number of events to transfer at once.  Higher values are better
# but as with queue store sizes require more memory.
replicator.thl.protocol.buffer_size=10

# THL listener address for remote access.  To listen on all ports, 
# use a value like thl://0.0.0.0:2112/.  (2112 is default
# port for Tungsten configurations.)  Note that this URI must provide
# a listener that corresponds to the URL in replicator.master.listen.uri, or 
# slaves may connect to the wrong port. 
replicator.store.thl.storageListenerUri=thl://<%= ENV['OPENSHIFT_TUNGSTEN_IP'] %>:<%= ENV['OPENSHIFT_TUNGSTEN_REPL_MASTER_LISTEN_PORT'] %>/

# The transfer rate between the master and the slave can be improved
# by setting the following setting to a value greater than 1, which
# is the default.  In doing so, this will also require more memory
# on the master side.
replicator.store.thl.resetPeriod=1

# Do not allow this replication service to make any changes to the THL files
replicator.store.thl.readOnly=<%= ENV['TUNGSTEN_REPL_SVC_THL_READ_ONLY'] %>

# Allow this replication service to keep on storing THL events even if it failed
# to store its position into database. This is not activated by default and 
# does require that THL syncTHLWithExtractor property is set to true (master case).
replicator.store.thl.stopOnDBError=true

#################################
# IN-MEMORY QUEUE STORE         #
#################################

# In-memory storage to buffer events between stages.
replicator.store.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueStore
replicator.store.queue.maxSize=${replicator.global.buffer.size}

# Parallel queue storage. 
replicator.store.parallel-queue=<%= ENV['TUNGSTEN_REPL_SVC_PARALLELIZATION_STORE_CLASS'] %>
replicator.store.parallel-queue.maxSize=${replicator.global.buffer.size}
replicator.store.parallel-queue.partitions=${replicator.global.apply.channels}
replicator.store.parallel-queue.partitionerClass=com.continuent.tungsten.replicator.storage.parallel.ShardListPartitioner
replicator.store.parallel-queue.maxOfflineInterval=5
replicator.store.parallel-queue.syncInterval=10000

##############
# EXTRACTORS #
##############

<%= ENV['TUNGSTEN_REPL_SVC_EXTRACTOR_CONFIG'] %>

# Local THL extractor. 
replicator.extractor.thl-extractor=com.continuent.tungsten.replicator.thl.THLStoreExtractor
replicator.extractor.thl-extractor.storeName=thl

# Remote THL extractor. 
replicator.extractor.thl-remote=com.continuent.tungsten.replicator.thl.RemoteTHLExtractor
replicator.extractor.thl-remote.connectUri=${replicator.master.connect.uri}

# If true, check to ensure logs are consistent on connection to master by 
# comparing epoch numbers of last committed transaction.  This can be 
# temporarily overridden when going online using 'trepctl online -force'.
replicator.extractor.thl-remote.checkSerialization=true

# Set requested interval in milliseconds for heartbeat events from remote THL.
# This may need to be adjusted higher if masters are very slow to respond or
# to scan the THL for slave start positions. 
replicator.extractor.thl-remote.heartbeatInterval=3000

# Set the number of seconds between retrying to master(s) after a lost
# connection.
replicator.extractor.thl-remote.retryInterval=1

# Set preferred THL server role.  Common values are master, slave, or empty
# (i.e., no value).  When replicating from a Tungsten cluster 'slave' avoids
# problems with failover when transactions are trapped on an old master. 
replicator.extractor.thl-remote.preferredRole=<%= ENV['TUNGSTEN_REPL_MASTER_PREFERRED_ROLE'] %>

# Set the number of seconds to find the preferred role when reconnecting.  
# After this expires the replicator will choose any available master. 
replicator.extractor.thl-remote.preferredRoleTimeout=30

# Queue extractor.
replicator.extractor.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueAdapter
replicator.extractor.queue.storeName=queue

# Parallel queue extractor. 
replicator.extractor.parallel-q-extractor=<%= ENV['TUNGSTEN_REPL_SVC_PARALLELIZATION_EXTRACTOR_CLASS'] %>
replicator.extractor.parallel-q-extractor.storeName=parallel-queue

############
# APPLIERS #
############

<%= ENV['TUNGSTEN_REPL_SVC_APPLIER_CONFIG'] %>

# Local THL applier. 
replicator.applier.thl-applier=com.continuent.tungsten.replicator.thl.THLStoreApplier
replicator.applier.thl-applier.storeName=thl

# Queue applier.
replicator.applier.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueAdapter
replicator.applier.queue.storeName=queue

# Parallel queue applier. 
replicator.applier.parallel-q-applier=<%= ENV['TUNGSTEN_REPL_SVC_PARALLELIZATION_APPLIER_CLASS'] %>
replicator.applier.parallel-q-applier.storeName=parallel-queue

###########
# FILTERS # 
###########

<%= ENV['TUNGSTEN_REPL_SVC_FILTER_CONFIG'] %>

#####################
# PIPELINE SERVICES #
#####################

# Channel service to manage shard-to-channel assignment in parallel
# replication.  Must be enabled to use round-robin assignment in
# shard.list.  The number of configured channels must be correctly configured
# to avoid replication errors. 
replicator.service.channel-assignment=com.continuent.tungsten.replicator.channel.ChannelAssignmentService
replicator.service.channel-assignment.url=<%= ENV['TUNGSTEN_APPLIER_REPL_DBTHLURL'] %>
replicator.service.channel-assignment.channels=${replicator.global.apply.channels}

################################
# BACKUP/RESTORE CONFIGURATION #
################################

# List of configured backup agents.  Uncomment appropriately for your site. 
replicator.backup.agents=<%= ENV['TUNGSTEN_APPLIER_REPL_DBBACKUPAGENTS'] %>

# Default backup agent.
replicator.backup.default=<%= ENV['TUNGSTEN_REPL_BACKUP_METHOD'] %>

<%= ENV['TUNGSTEN_REPL_SVC_BACKUP_CONFIG'] %>

# List of configured storage agents.  Uncomment appropriately for your site. 
replicator.storage.agents=fs

# Default storage agent.
replicator.storage.default=fs

# File system storage agent.  For best results the directory parameter should
# be a shared file system visible to all replicators.  NOTE: CRC file checking
# may be time-consuming for large files; it is recommended if you can afford
# to check.  (Who really wants to load a bad backup??)
replicator.storage.agent.fs=com.continuent.tungsten.replicator.backup.FileSystemStorageAgent
replicator.storage.agent.fs.directory=<%= ENV['TUNGSTEN_SERVICE_REPL_BACKUP_STORAGE_DIR'] %>
replicator.storage.agent.fs.retention=<%= ENV['TUNGSTEN_REPL_BACKUP_RETENTION'] %>
replicator.storage.agent.fs.crcCheckingEnabled=true

####################################################
# ERROR-HANDLING AND CONSISTENCY-CHECKING POLICIES #
####################################################

# How to react on extractor failure. Possible values are 'stop' or 'warn'. 
replicator.extractor.failure_policy=stop

# How to react on applier failure. Possible values are 'stop' or 'warn'. 
replicator.applier.failure_policy=stop

# How applier should react when detecting a row update that did not change anything.
# Possible values are 'stop', 'warn' or 'ignore'. 
replicator.applier.failOnZeroRowUpdate=warn

# How to react on consistency check failure.  Possible values are 'stop' or 
# 'warn'. 
replicator.applier.consistency_policy=<%= ENV['TUNGSTEN_REPL_CONSISTENCY_POLICY'] %>

# Should consistency check be sensitive to column names and/or types? Settings
# on a slave must be identical to master's. Values are 'true' or 'false'.
replicator.applier.consistency_column_names=true
# TUC-20:  Must disable column type checking when going from
# Connector/J on master to drizzle on slave.
replicator.applier.consistency_column_types=false
